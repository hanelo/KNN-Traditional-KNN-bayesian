{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Chargement du jeu de données Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Séparation des données en ensembles d'entraînement et de test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialisation et apprentissage du modèle k-NN\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n# Prédiction sur les données de test\ny_pred = knn.predict(X_test)\n\n# Calcul de l'exactitude du modèle\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Exactitude du modèle k-NN : {:.2f}\".format(accuracy))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T08:56:15.056414Z","iopub.execute_input":"2023-05-23T08:56:15.057120Z","iopub.status.idle":"2023-05-23T08:56:15.330114Z","shell.execute_reply.started":"2023-05-23T08:56:15.057083Z","shell.execute_reply":"2023-05-23T08:56:15.328903Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Exactitude du modèle k-NN : 1.00\n","output_type":"stream"}]},{"cell_type":"markdown","source":"L'algorithme d'inférence variationnelle est une méthode d'approximation utilisée pour estimer les distributions postérieures dans les modèles probabilistes.Cette approche permet de simplifier le problème d'estimation en remplaçant la distribution postérieure difficile à calculer par une distribution plus facile à manipuler.\nL'objectif est de trouver les paramètres de cette distribution approximative qui minimisent la divergence entre la distribution approximative et la distribution postérieure réelle.\n\nL'inférence variationnelle est une approche couramment utilisée lorsque la distribution postérieure exacte est inconnue ou intractable, et elle offre une alternative efficace aux méthodes d'échantillonnage plus coûteuses en termes de calcul, telles que l'échantillonnage de Monte Carlo par chaînes de Markov (MCMC).\n\nprincipe:\n\n1-Modèle probabiliste : Tout d'abord, on définit un modèle probabiliste qui décrit la relation entre les variables observées et les variables latentes inconnues. Ce modèle est généralement basé sur des hypothèses probabilistes et peut prendre la forme d'un modèle graphique, d'un modèle bayésien, etc.\n\n2-Distribution postérieure : L'objectif est d'estimer la distribution postérieure des variables latentes étant donné les observations. Cependant, calculer exactement cette distribution peut être difficile, voire impossible, en raison de l'intractabilité mathématique ou de la complexité du modèle.\n\n3-Distribution approximative : On choisit une distribution approximative appartenant à une famille paramétrique plus restreinte qui sera plus facile à manipuler. Cette distribution est généralement sélectionnée en fonction de sa flexibilité et de sa capacité à s'adapter au modèle.\n\n4-Minimisation de la divergence de KL : L'objectif est de trouver les paramètres de la distribution approximative qui minimisent la divergence de KL entre la distribution approximative et la distribution postérieure réelle. Cela revient à trouver les meilleurs paramètres qui \"rapprochent\" la distribution approximative de la distribution postérieure.\n\n5-Algorithme d'optimisation : Pour minimiser la divergence de KL, on utilise des techniques d'optimisation numérique telles que la descente de gradient, l'optimisation par coordonnées ou l'optimisation stochastique. L'algorithme d'optimisation ajuste itérativement les paramètres de la distribution approximative jusqu'à ce qu'une convergence satisfaisante soit atteinte.\n\n6-Estimation de la distribution postérieure : Une fois que les paramètres de la distribution approximative ont été optimisés, on utilise cette distribution approximative pour estimer la distribution postérieure des variables latentes. Cette estimation est une approximation de la distribution postérieure réelle, mais elle peut être utilisée pour effectuer des prédictions, des analyses statistiques ou d'autres tâches d'inférence.\n\n.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pymc3 as pm\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n# Charger le jeu de données Iris\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Prétraitement des étiquettes en encodage entier\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n# Fractionnement des données en ensembles d'entraînement et de test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndef bayesian_knn(x_train, y_train, x_test, k):\n    n_train = x_train.shape[0]\n    n_test = x_test.shape[0]\n\n    with pm.Model() as model:\n        # Prior pour les classes\n        p = pm.Dirichlet('p', a=np.ones(len(np.unique(y_train))), shape=len(np.unique(y_train)))\n\n        # Calcul des distances entre les échantillons d'entraînement et les échantillons de test\n        dists = np.sum((x_train[:, np.newaxis] - x_test) ** 2, axis=2)\n\n        # Extraction des indices des k plus petites distances pour chaque échantillon de test\n        k_indices = np.argpartition(dists, k, axis=0)[:k]\n\n        # Extraction des étiquettes correspondantes aux indices des k plus petites distances\n        k_labels = y_train[k_indices]\n\n        # Définition des variables catégorielles likelihood pour chaque échantillon de test\n        for i in range(n_test):\n            likelihood = pm.Categorical(f'likelihood_{i}', p=p[k_labels[:, i]], observed=k_labels[:, i])\n\n        # Inférence variationnelle\n        approx = pm.fit(n=500, method='advi')\n\n        # Échantillonnage à partir de la distribution approximative\n        trace = approx.sample(500)\n\n    # Réduire la dimension de y_pred_bayesian pour avoir la même longueur que y_test\n    y_pred_bayesian = np.argmax(trace['p'], axis=1)[:len(y_test)]\n\n    # Calcul de la précision du k-NN bayésien\n    accuracy_bayesian = accuracy_score(y_test, y_pred_bayesian)\n\n    return model, approx, accuracy_bayesian\n\n# Utilisation de l'algorithme de k-NN bayésien avec inférence variationnelle sur les données d'entraînement et de test\nmodel, approx, accuracy = bayesian_knn(X_train, y_train, X_test, k=3)\n\nprint(\"Précision du k-NN bayésien :\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-23T08:53:32.521627Z","iopub.execute_input":"2023-05-23T08:53:32.522085Z","iopub.status.idle":"2023-05-23T08:56:15.054504Z","shell.execute_reply.started":"2023-05-23T08:53:32.522043Z","shell.execute_reply":"2023-05-23T08:56:15.053518Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='500' class='' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [500/500 00:00&lt;00:00 Average Loss = 100.2]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Précision du k-NN bayésien : 0.3333333333333333\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}